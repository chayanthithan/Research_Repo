{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a64358e-cb50-42ed-877f-dad3de00bb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ba60fdd-a468-41c3-9b9d-d2ee19863b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ad8eca-7e8a-4648-9dc7-716eb12022f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36787ac6-0f3f-4ae4-bfe1-d9b869fcaa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"D:/Academic/4th year/Research/NewDataset\"  # Update with the correct path\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "test_dir = os.path.join(dataset_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df46c86d-401b-475e-a127-ec8c357aa322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image data generators for augmentation and preprocessing\n",
    "img_height, img_width = 150, 150\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5982659-db97-42ef-8248-f8bede425a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6954 images belonging to 13 classes.\n",
      "Found 1733 images belonging to 13 classes.\n",
      "Found 2170 images belonging to 13 classes.\n"
     ]
    }
   ],
   "source": [
    "# Using validation_split to split training data into training and validation sets\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, \n",
    "                                   rotation_range=20, \n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   validation_split=0.2)  # 20% for validation\n",
    "# Training data generator\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                     target_size=(img_height, img_width),\n",
    "                                                     batch_size=batch_size,\n",
    "                                                     class_mode='categorical',\n",
    "                                                     subset='training')  # Specify training subset\n",
    "# Validation data generator\n",
    "val_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                   target_size=(img_height, img_width),\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   class_mode='categorical',\n",
    "                                                   subset='validation')  # Specify validation subset\n",
    "# Test data generator\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(test_dir,\n",
    "                                                   target_size=(img_height, img_width),\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   class_mode='categorical',\n",
    "                                                   shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6a91fd7-53f9-47d8-b27e-81f78f5fa614",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Define CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(13, activation='softmax')  # 13 classes\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2f58c7f-b0d9-4a65-a830-31a0b1d3902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bb9a3e1-2a18-44b1-8ee3-ddc098feb98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7s/step - accuracy: 0.3517 - loss: 1.8873"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2174s\u001b[0m 10s/step - accuracy: 0.3524 - loss: 1.8854 - val_accuracy: 0.6549 - val_loss: 1.1121\n",
      "Epoch 2/30\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1912s\u001b[0m 9s/step - accuracy: 0.7119 - loss: 0.8508 - val_accuracy: 0.7571 - val_loss: 0.9191\n",
      "Epoch 3/30\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1861s\u001b[0m 9s/step - accuracy: 0.7887 - loss: 0.6076 - val_accuracy: 0.6844 - val_loss: 1.1394\n",
      "Epoch 4/30\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1530s\u001b[0m 7s/step - accuracy: 0.8369 - loss: 0.4632 - val_accuracy: 0.7634 - val_loss: 0.8798\n",
      "Epoch 5/30\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1534s\u001b[0m 7s/step - accuracy: 0.8658 - loss: 0.4147 - val_accuracy: 0.7744 - val_loss: 0.9297\n",
      "Epoch 6/30\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1536s\u001b[0m 7s/step - accuracy: 0.8896 - loss: 0.3411 - val_accuracy: 0.7877 - val_loss: 0.8994\n",
      "Epoch 7/30\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1519s\u001b[0m 7s/step - accuracy: 0.8964 - loss: 0.3227 - val_accuracy: 0.7640 - val_loss: 1.2053\n",
      "Epoch 8/30\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2322s\u001b[0m 11s/step - accuracy: 0.9151 - loss: 0.2626 - val_accuracy: 0.7957 - val_loss: 0.8276\n",
      "Epoch 9/30\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1520s\u001b[0m 7s/step - accuracy: 0.9197 - loss: 0.2560 - val_accuracy: 0.7807 - val_loss: 0.9139\n",
      "Epoch 10/30\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7235s\u001b[0m 33s/step - accuracy: 0.9267 - loss: 0.2457 - val_accuracy: 0.7865 - val_loss: 0.8961\n",
      "Epoch 11/30\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1993s\u001b[0m 9s/step - accuracy: 0.9262 - loss: 0.2310 - val_accuracy: 0.7778 - val_loss: 1.0942\n",
      "Epoch 12/30\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2007s\u001b[0m 9s/step - accuracy: 0.9202 - loss: 0.2483 - val_accuracy: 0.8096 - val_loss: 0.9674\n",
      "Epoch 13/30\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2760s\u001b[0m 13s/step - accuracy: 0.9491 - loss: 0.1570 - val_accuracy: 0.7669 - val_loss: 1.2605\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Train the model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "                    validation_data=val_generator,\n",
    "                    epochs=30,\n",
    "                    callbacks=[early_stopping]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd231aae-f3cb-44f8-bd97-ce99f0fea710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m448s\u001b[0m 7s/step - accuracy: 0.6844 - loss: 2.0997\n",
      "Test Accuracy: 0.67\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a7ad2e-11c1-4318-8887-c078f59807b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"leaf_identifyer.h5\"\n",
    "model.save(model_save_path)\n",
    "print(f\"Model saved at {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad5b21e-45a0-48b6-8bb1-00b94a7270e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Plot accuracy\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803c1dc7-4f3e-42a5-a7dc-ad5df3039214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "def load_and_test_model(image_path):\n",
    "    loaded_model = load_model(model_save_path)\n",
    "    from tensorflow.keras.preprocessing import image\n",
    "    img = image.load_img(image_path, target_size=(img_height, img_width))\n",
    "    img_array = image.img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    prediction = loaded_model.predict(img_array)\n",
    "    class_indices = {v: k for k, v in train_generator.class_indices.items()}\n",
    "    predicted_class = class_indices[np.argmax(prediction)]\n",
    "    print(f\"Predicted leaf type: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a40ade4-4d30-49eb-86a0-5bacc459b12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'g2.jpg'\n",
    "load_and_test_model(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a647bd72-06aa-4d61-84b1-46eaa96aa934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21516176-6c50-4b82-a5c9-6d3c0c70cf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\cinnamon_leaf_1.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\cinnamon_leaf_10.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\cinnamon_leaf_2.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\cinnamon_leaf_3.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\cinnamon_leaf_4.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\cinnamon_leaf_5.jpeg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\cinnamon_leaf_6.jpeg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\cinnamon_leaf_7.jpeg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\cinnamon_leaf_8.jpeg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\cinnamon_leaf_9.jpeg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\coffee.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\coffee6.jpeg\n",
      "Skipping non-image file: coffee_2.avif\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\coffee_3.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\coffee_4.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\coffee_5.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\ebony_1.png\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\grape_1.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\grape_2.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\grape_3.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\grape_4.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\grape_5.png\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\jack_leaf_11.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\jack_leaf_2.jpeg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\jack_leaf_4.jpeg\n",
      "Skipping non-image file: jack_leaf_6.webp\n",
      "Skipping non-image file: jack_leaf_7.webp\n",
      "Skipping non-image file: jack_leaf_8.webp\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\mango_1.png\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\mango_2.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\mango_3.jpeg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\mango_4.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\mango_5.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\pepper_leaf_1.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\pepper_leaf_10.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\pepper_leaf_2.jpeg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\pepper_leaf_3.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\pepper_leaf_4.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\pepper_leaf_5.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\pepper_leaf_6.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\pepper_leaf_7.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\pepper_leaf_8.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\pepper_leaf_9.jpg\n",
      "Skipping non-image file: sandalwood_1.webp\n",
      "Skipping non-image file: sandalwood_2.webp\n",
      "Skipping non-image file: sandalwood_3.webp\n",
      "Skipping non-image file: sandalwood_4.webp\n",
      "Skipping non-image file: sandalwood_5.webp\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\tea.jpeg\n",
      "Skipping non-image file: tea1.avif\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\teak_1.jpg\n",
      "Skipping non-image file: teak_2.webp\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\teak_3.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\teak_4.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\teak_5.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "Saved annotated image: D:/Academic/4th year/Research/results\\teak_6.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Constants\n",
    "model_save_path = \"leaf_identifyer.h5\"\n",
    "img_height, img_width = 224, 224  # Ensure these match your model's input size\n",
    "output_folder = \"D:/Academic/4th year/Research/results\"  # Folder to save annotated images\n",
    "\n",
    "# Load the trained model\n",
    "def load_and_save_predictions(folder_path):\n",
    "    # Load the saved model\n",
    "    loaded_model = load_model(model_save_path)\n",
    "    \n",
    "    # Reverse the class indices to map predictions to class names\n",
    "    class_indices = {v: k for k, v in train_generator.class_indices.items()}\n",
    "    \n",
    "    # Create output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Iterate over all files in the folder\n",
    "    for img_file in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, img_file)\n",
    "        \n",
    "        if not img_file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff')):\n",
    "            # Skip files that are not images\n",
    "            print(f\"Skipping non-image file: {img_file}\")\n",
    "            continue\n",
    "        \n",
    "        # Load and preprocess the image\n",
    "        img = image.load_img(img_path, target_size=(img_height, img_width))\n",
    "        img_array = image.img_to_array(img) / 255.0\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        \n",
    "        # Predict the class\n",
    "        prediction = loaded_model.predict(img_array)\n",
    "        predicted_class = class_indices[np.argmax(prediction)]\n",
    "        \n",
    "        # Annotate and save the image\n",
    "        save_annotated_image(img_path, img_file, predicted_class)\n",
    "\n",
    "# Function to annotate and save the image\n",
    "def save_annotated_image(img_path, img_file, predicted_class):\n",
    "    # Open the original image\n",
    "    original_img = Image.open(img_path)\n",
    "    \n",
    "    # Create a drawing context\n",
    "    draw = ImageDraw.Draw(original_img)\n",
    "    \n",
    "    # Define font (use default if custom font isn't available)\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", size=20)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "    \n",
    "    # Add the prediction text\n",
    "    text = f\"Prediction: {predicted_class}\"\n",
    "    text_position = (10, 10)  # Top-left corner\n",
    "    text_color = (255, 0, 0)  # Red color\n",
    "    \n",
    "    draw.text(text_position, text, fill=text_color, font=font)\n",
    "    \n",
    "    # Save the annotated image in the output folder\n",
    "    output_path = os.path.join(output_folder, img_file)\n",
    "    original_img.save(output_path)\n",
    "    print(f\"Saved annotated image: {output_path}\")\n",
    "\n",
    "# Folder path containing test images\n",
    "folder_path = 'D:/Academic/4th year/Research/test'\n",
    "load_and_save_predictions(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fc755e-c5b8-47fe-8ab4-23373aac3fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
